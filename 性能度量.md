# 回归与分类模型的性能度量

性能度量(performance measure), 是衡量模型泛化能力的评价标准

性能度量反映任务需求，在对比不同模型的能力时，使用不同的性能度量会导致不同的评判结果

## 2.2 性能度量-回归问题和度量

回归问题的目标是在给定$d$维输入变量$x$的情况下，预测一个或者多个连续目标变量$y$的值

给定样例集合$D$ = {$(x_1,y_1),...,(x_n,y_n)$},评估学习器$f$的性能

常用性能度量为“均方误差”：
$$
E(f;D) = \frac{1}{m}\Sigma_{i=1}^{m}(f(x_i)-y_i)^2
$$
更一般地，对于数据集$D$和概率分布$p(x,y)$有期望损失(expected square prediction error)：
$$
EPE(f) = \int\int(f(x)-y)^2p(x,y)dxdy
$$
将$X$视为条件变量,得到期望损失的另一种形式：
$$
EPE(f) = E_XE_{Y|X}[(f(X)-Y)^2|X = x]
$$
其中
$$
\begin{array}{ll}
E_{Y|X}[(f(X)-Y)^2|X = x]\\
= E_{Y|X}[(Y-E[Y|X=x])^2] + \{E_{Y|X}[Y|X=x] - f(x)\}^2\\
= var(Y|X = x) + (E_{Y|X}[Y|X = x]- f(x))^2
\end{array}
$$
因此$f(x) = E_Y(Y|X = x)$时，期望损失有最小值$E[(Y - E[Y])^2] = var(Y)$

## 2.3 二分类问题的错误率与精度

二分类问题中$y_i$取值为{0,1}，因此不再关注$y$的分布函数

* 等权错误率：
	$$
	E(f,D) = \frac{1}{N}\Sigma_{i=1}^{N}\mathbb I[f(x_i) \not = y_i]
	$$
	
* 等权精度：
	$$
	ACC(f,D) = \frac{1}{N}\Sigma_{i=1}^{N}\mathbb I[f(x_i) = y_i] = 1- E(f,D)
	$$

* 一般错误率：
	$$
	E(f,D) = \frac{1}{N}\int_{x \in D} \mathbb I[f(x) \not = y]p(x)dx
	$$

* 一般精度：
	$$
	E(f,D) = \frac{1}{N}\int_{x \in D} \mathbb I[f(x) = y]p(x)dx
	$$
	

## 2.4 查准率与查全率

* 查准率P表示在所有预测结果为正的样例中，正确分类的样例数量所占比例。因为考虑总体是预测结果为正的样例，考察准确率，因此称为查准率

* 查全率R(也称召回率)表示真实情况为正例的样例中，正确分类的样例数量所占比例。因为考虑总体是真实情况为正的样例，考察召回率（分类为正例），因此称为查全率

	![image-20200406115800688](C:\Users\mi\AppData\Roaming\Typora\typora-user-images\image-20200406115800688.png)

$$
Precision = \frac{TP}{TP + FP}\\
Recall = \frac{TP}{TP + FN}\\
$$

* 通常情况下，查全率和查准率反向变动

	![image-20200406120803862](C:\Users\mi\AppData\Roaming\Typora\typora-user-images\image-20200406120803862.png)

	元组为样例集$D$中的样例，分类算法将元组全部映射为正例，从左到右样例集中的正例数目减少，查准率减小，查全率增大

	![image-20200406123717093](C:\Users\mi\AppData\Roaming\Typora\typora-user-images\image-20200406123717093.png)

### 2.4.1 P-R曲线

* 以查准率$P$为纵轴，查全率$R$为横轴作图，可以得到$P-R$曲线

	![查看源图像](https://tse2-mm.cn.bing.net/th?id=OIP.VojSG4YiGk01UCT-KbxEdQHaG9&pid=Api&rs=1)

	* 现实任务中的P-R曲线是非单调、不平滑的，在很多局部有上下波动

	* 若一个学习器的P-R曲线被另一个学习器的曲线完全“包住”，则可断言后者性能优于前者

	* 如果两个学习器的P-R曲线发生了交叉，则难以一般性地断言两者性能优劣，特此人们设计了一些综合考虑查准率、查全率的性能度量：

		* P-R曲线下的面积：

			​	一定程度上表征了学习器在查准率和查全率上取得双高的比例，但是难以估算

		* “平衡点(BEP: Break-Even Point)”:

			​	查准率等于查全率时的取值，但是过于简化导致损失信息

		* F1度量：基于查准率与查全率的调和平均，更重视较小值
			$$
			\frac{1}{F_1} = \frac{1}{2}*(\frac{1}{P}+\frac{1}{R})\\
			F_1 = \frac{2*P*R}{P+R}
			$$
			​	

		* $F_{\beta}$度量：
			$$
			\frac{1}{F_\beta} = \frac{1}{1+\beta^2}*(\frac{1}{P}+\frac{\beta^2}{R})
			$$
			$\beta > 1$时查全率有更大影响；$\beta < 1$时查准率有更大影响

* eg.对于图中的两个图，判断线性模型划分得到的P-R曲线关系

	![image-20200406125013224](C:\Users\mi\AppData\Roaming\Typora\typora-user-images\image-20200406125013224.png)

	solution：类似BEP法，绘制如下分布图，计算并比较$FP = FN$时的查准率与查全率，得到P-R的大小关系。左边的P-R曲线在右边P-R曲线之内

	![image-20200406123717093](C:\Users\mi\AppData\Roaming\Typora\typora-user-images\image-20200406123717093.png)

* 基于多次试验得到的混淆矩阵综合考察查准率和查全率

	* 在各个混淆矩阵上分别计算查准率和查全率，记为$(P_1,R_1),...,(P_n,R_n)$
		$$
		macro-P = \frac{1}{n}\Sigma_{i=1}^{n}P_i \\
		macro-R = \frac{1}{n}\Sigma_{i=1}^{n}R_i \\
		macro-F_1 =\frac{2*macro-P*macro-R}{macro-P+macro-R}
		$$
	
* 计算$TP、FP、TN、FN$的平均值$\bar {TP}、\bar {FP}、\bar {TN}、\bar{FN}$
		$$
		micro-P = \frac{\bar {TP}}{\bar {TP}+\bar {FP}} \\
		micro-R = \frac{\bar {TP}}{\bar {TP}+\bar {FN}} \\
		micro-F_1 =\frac{2*micro-P*micro-R}{micro-P+micro-R}
	$$

### 2.4.2 ROC与AUC曲线

* 分类学习器的基本思想：为测试样本产生一个实值或概率预测，然后将这个预测值与一个分类阈值进行比较，若大于阈值则分为正类，否则为负类

* 阈值的好坏直接决定了学习器的泛化能力，根据这个阈值可以将测试样本排序，“最可能”是正例的排在最前面，“最不可能”是正例的排在最后面，分类过程就相当于在阈值位置将前一部分判作正例，后一部分判作反例。

* 靠前位置截断，P偏大，R偏小；靠后位置截断，P偏小，R偏大

* 计算“真正例率TPR”与“假正例率（FPR）”绘制ROC曲线
	$$
	TPR = \frac{TP}{TP +FN}\\
	FPR = \frac{FP}{TN + FP}
	$$
	![查看源图像](https://tse4-mm.cn.bing.net/th?id=OIP.eoKWPP4oFG8Z-oIqmZsCOwAAAA&pid=Api&rs=1)

	* 对角线对应于“随机猜测模型”：以相同概率预测正例与反例
	* （0,1）表示理想模型，即按照该模型产生的概率预测排列样例，能保证所有正例排在反例之前。在真实情况下，模型会将一些反例排在正例之前
	* 现实任务中利用有限个测试样例绘制ROC图，此时仅能获得有限个坐标对，无法产生光滑ROC曲线

![查看源图像](http://scikit-learn.org/stable/_images/sphx_glr_plot_roc_001.png)

* 绘制过程：

	给定m+个正例和m-个反例，根据学习器预测结果对样例排序，将分类阈值设为最大(将所有正例预测为反例，位于(0,0)）。

	然后将分类阈值依次设为每个样例的预测值，依次将每个样例划分为正例（反例样例可能被划分为正例）。

	设前一个标记点(以其概率估计作为阈值)坐标为(x,y)，若当前为真正例，则FPR不变，TPR分母不变分子加一，得到$(x,y+\frac{1}{m+})$；若当前为假正例，TPR不变，FPR分母不变分子加一，则对应标记点的坐标为$(x+\frac{1}{m-},y)$

* 比较原则

	* 若一个学习器的ROC曲线包住另一个学习器的ROC曲线，则前者性能优于后者

	* 若两个学习器的ROC曲线相交，则比较ROC曲线下的面积AUC(Area Under ROC Curve)

		* AUC估算公式：
			$$
			AUC = \frac{1}{2}\Sigma_{i=1}^{m-1}(x_{i+1}-x_{i})*(y_i+y_{i+1})
			$$

		* 损失：
			$$
			l_{rank} = \frac{1}{m^+m^-}\Sigma_{x^+ \in D^+}\Sigma_{x^- \in D^-}(\mathbb I[f(x^+)<f(x^-)] + \frac{1}{2}\mathbb I[f(x^+) = f(x^-)])
			$$

			1. 考虑每一对正、反例，若正例预测值小于等于反例，则记罚分

			2. 因为每一个正例在ROC曲线上对应标记点坐标(x,y)的x表示排序在其之前的反例所占比例，则$l_{rank}(x) = x$:
				$$
				\begin{array}{ll}
				l_{rank} \\
				 = \Sigma_{x^+ \in D^+}l_{rank}(x) \\
				 = \frac{1}{m^+m^-}\Sigma_{x^+ \in D^+}\Sigma_{x^- \in D^-}(\mathbb I[f(x^+)<f(x^-)] + \frac{1}{2}\mathbb I[f(x^+) = f(x^-)])\\
				 = 1 - AUC
				 \end{array}
				$$
				恰好是m+个横向曲边梯形面积之和

* ROC曲线的优质特性

	* 当测试集中的正负样本分布不对等时，ROC曲线能够保持不变，P-R曲线发生显著变化

		![image-20200406140222036](C:\Users\mi\AppData\Roaming\Typora\typora-user-images\image-20200406140222036.png)

		左侧为ROC曲线，右侧为P-R曲线

		上方为正负样本分布平衡，下方为正负样本分布不均

### 2.4.3 代价敏感错误率与代价曲线

* 为权衡不同类型错误所造成的不同损失，可为错误赋予“非均等代价”

* 代价矩阵：

	![image-20200406142058361](C:\Users\mi\AppData\Roaming\Typora\typora-user-images\image-20200406142058361.png)

* 代价敏感错误率：
	$$
	E(f;D;cost) = \frac{1}{m}(\Sigma_{x_i \in D^+}\mathbb I[f(x_i) \not = y_i]*cost_{01} + \Sigma_{x_i \in D^-} \mathbb I[f(x_i)*cost_{10}])
	$$
	
* 代价曲线与ROC曲线

  * 在非均等代价的前提下，代价曲线不能直接反映出机器学习器的总体代价，而“代价曲线”可以达到此目标

  * 出于模型泛化的考虑，此处的正例率$p$不是某个测试集上的正例率，而是对于所有可能测试集上的正例率的先验概率（不同测试集上的正例率服从$U[0,1]$）

  * 由此得到一个分类器的期望代价：
  	$$
  	E[cost] = FNR*p(+)*cost_{10} + FPR*p(-)*cost_{01}
  	$$
  	最大期望代价在所有实例都被错误分类的情况下出现：
  	$$
  	max E[cost] = p(+)*cost_{10} + p(-)*cost_{01}
  	$$
  	
* 代价曲线横轴为“正例概率代价”，纵轴是“归一化代价
  	$$
  	P_{(+)cost} = \frac{p*cost_{01}}{p*cost_{01}+(1-p)*cost_{10}}\\
  	cost_{norm} = \frac{FNR*p*cost_{01}+FPR*(1-p)*cost_{10}}{p*cost_{01}+(1-p)*cost_{10}}
  	$$
  	相应地，存在“负例概率代价”：
  	$$
  	P_{(-)cost} = \frac{(1-p)*cost_{10}}{p*cost_{01}+(1-p)*cost_{10}}\\
  	$$
  	
  
* 代价曲线的绘制：

	* 将ROC曲线上的点(FPR,TPR)映射到代价曲线的一条从(0,FPR)到(1,FPR)的边界，边界的公共下界围成的面积即为在所有条件下学习器的期望总代价。

	![image-20200406143555256](C:\Users\mi\AppData\Roaming\Typora\typora-user-images\image-20200406143555256.png)
	
	* 当正例率$p$的先验分布服从$U[0,1]$时，期望总体代价恰好等于代价曲线之下的曲边梯形的面积之和
	
	* 若正例率$p$的先验分布服从某一特定分布并知道其概率密度函数$prob(x)$时，其期望代价为：
		$$
		TEC = \int_{0}^1 Norm(E[Cost(x)])*prob(x)dx
		$$
	
	 	  此时期望代价不一定是曲边梯形面积之和

## 2.5 比较检验

* 影响机器学习性能比较的因素
	1. 实验评估得到的是测试集上的性能，这与真实的泛化性能比较结果不一定相同
	2. 测试集的性能与测试集集本身的选择有很大的关系，测试集的大小，测试集的样例构成都会影响测试结果。
	3. 机器学习算法本身具有随机性，即便使用相同的参数设置在同一个测试集上多次运行，结果也会不同

### 2.5.1 二项检验

假设实验次数为$K$，每次实验产生一个泛化误差估计$\hat {\epsilon_k}$，作为真实泛化误差$\epsilon$的估计

以当前的模型泛化误差$\epsilon$在依次测试中产生泛化误差$\hat{\epsilon_k}$的概率为
$$
P(\hat{\epsilon_k}|\epsilon) = \binom{n_k}{mc_k}\epsilon^{mc_k}(1-\epsilon)^{n_k - mc_k}
$$
其中$\hat{\epsilon_k} = \frac{mc_k}{n_k}$

* 检验步骤：

	1. 给出一个待比较的泛化误差$\epsilon_0$，信度$\alpha > 0$，取$k^* = \arg{max\hat{\epsilon_k}}$ 

	2. 找到产生最大错误率的实验，对这组错误率设立检验：$H_0:\epsilon \leq \epsilon_0 $以及$H_1:\epsilon > \epsilon_0$

	3. 计算概率
		$$
		P(mc>mc_{k*}|\epsilon_0) = \Sigma_{i = mc_{k^*}+1}^{n_{k^*}}\binom{n_{k^*}}{i}\epsilon_0^i(1-\epsilon_0)^{n-i}
		$$
		
	4. 若$P(mc>mc_{k*}|\epsilon_0) < \alpha$则拒绝零假设， 可以在$\alpha$的显著性下认为学习器的泛化误差是大于$\epsilon_0$的

* e.g.假设错误率最高一组的误分类数是5，计算$1- pbinom_{cumulative}(5,10,0.3) = 0.047 > 0.05 = \alpha$则拒绝零假设，泛化误差超过0.3

### 2.5.2 多次留出以及交叉验证中的t检验

* 检验步骤：

	1. 设置假设：$H_0:\epsilon \leq \epsilon_0;H_1:\epsilon>\epsilon_0$

	2. 计算K次测试产生的错误率$\epsilon_1,...,\epsilon_K$的均值与方差
		$$
		\hat{\mu_{\epsilon}} = \frac{1}{k}\Sigma_{k=1}^K\epsilon_k \\
		\hat{\sigma}^2 = \frac{1}{k-1}\Sigma_{k=1}^K(\epsilon_k - \hat{\mu_{\epsilon}} )^2
		$$
		
3. 计算T统计量$T = \frac{\sqrt{K}(\hat{\mu_{\epsilon}} - \epsilon_0)}{\hat{\sigma}}$，服从自由度为K-1的t分布
	4. 若$P(T > t_{\alpha}(K-1)) < \alpha$则拒绝零假设，认为$\epsilon>\epsilon_0$

### 2.5.3 交叉验证t-检验

* 检验步骤：
	1. 对学习器A和B，使用K折交叉验证得到K个测试错误率$\epsilon_1^A,...,\epsilon_K^A$和$\epsilon_1^B,...,\epsilon_K^B$
	2. 设立双侧检验$H_0:\epsilon^A = \epsilon^B;H_1:\epsilon^A \not = \epsilon^B$
	3. 计算对应次测试错误率的差$\Delta_k = \epsilon_k^A - \epsilon_k^B$, 计算T统计量$T =  \frac{\sqrt{K}(\hat{\mu_{\Delta}} )}{\hat{\sigma_{\Delta}}}$
	4. 若$P(T > t_{\alpha/2}(K-1)) < \alpha$则拒绝零假设，认为$\epsilon^A \not = \epsilon^B$

*  问题：

	有效t检验的重要前提是测试误差率均为泛化误差的独立采样，然而由于样本有限，使用交叉验证等试验估计方法时，不同轮次训练集会出现重叠，使得测试误差率不独立，导致过高估计假设成立的概率

* 解决方式：5*2交叉验证法

	1. 每两次交叉验证之前用随机数将数据打乱，使得5次交叉验证的数据划分尽可能不重复
	2. 对两个学习器A和B，第i次2折交叉验证将产生两对测试错误率，分别求差，记为$\Delta_i^1、\Delta_i^2$
	3. 为缓解测试数据的非独立性，仅计算第一次2折交叉验证的结果的平均值$\mu = \frac{\Delta_1^1+\Delta_1^2}{2}$,但是对每次2折实验结果计算方差$\sigma_i^2 = \frac{(\Delta_i^1-\mu)^2 + (\Delta_i^2-\mu)^2}{2} $
	4. 计算T统计量$T = \frac{\mu}{\sqrt{\frac{\Sigma_{i=1}^5\sigma_i^2}{5}}}$， 服从自由度为5的t分布
	5. 若$P(T > t_{\alpha/2}(5)) < \alpha$则拒绝零假设，认为$\epsilon^A \not = \epsilon^B$

### 2.5.4 McNemar检验

![image-20200407225602338](C:\Users\mi\AppData\Roaming\Typora\typora-user-images\image-20200407225602338.png)

* 检验步骤：
	1. 若A,B学习器性能相同，则有$e_{01} = e_{10}$，$|e_{01} - e_{10}|$应服从正态分布
	2. 构造统计量$\chi^2 = \frac{(|e_{01} - e_{10}|)^2-1}{e_{01} + e_{10}}$, 服从自由度为1的卡方分布
	3. 若$P(\chi^2 > \chi^2_{\alpha}(1)) < \alpha$则拒绝零假设

### 2.5.5 Friedman检验和Nemenyi检验

![image-20200407233657854](C:\Users\mi\AppData\Roaming\Typora\typora-user-images\image-20200407233657854.png)

* 应用场景：多个数据集多个学习器进行比较时使用，对各个算法在各个数据集上对测试性能排序，对平均序值计算$\chi^2$和$F$检验，并进行临界值检验

* 假定在$N$个数据集上比较k个算法，$r_i$表示第$i$个算法的平均序值.暂不考虑平分序值的情形，有
	$$
	E[r_i] = \frac{k+1}{2}\\
	var[r_i] = \frac{k^2-1}{12N}
	$$
	

	变量
	$$
	\tau_{\chi^2} = \frac{12N}{k(k+1)}(\Sigma_{i=1}^{k}r_i^2-\frac{k(k+1)^2}{4}) \\
	Q_{\chi^2} = \frac{12\Sigma_{i=1}^kr_i^2}{Nk(k+1)} - 3N(K+1)
	$$
	在N*k比较大时，均服从$\Chi^2(k-1)$

	现在使用变量
	$$
	\tau_F = \frac{(N-1)\tau_{\chi^2}}{N(k-1)-\tau_{\chi^2}} \\
	\tau_F = \frac{(N-1)\Q_{\chi^2}}{N(k-1)-\Q_{\chi^2}} \\
	$$
	服从$F(k-1,(k-1)(N-1))$分布

	![image-20200407233732936](C:\Users\mi\AppData\Roaming\Typora\typora-user-images\image-20200407233732936.png)

* 当Friedman检验发现多个学习器之间性能存在差异时，想知道哪些算法性能差异较大，需要进行平均秩之差的Nemanyi后续检验，构造统计量：
	$$
	CD = q_{\alpha}\sqrt{\frac{K(K+1)}{6B}}
	$$

* e.g. $CD =q_{0.05}\sqrt{\frac{3*4}{6*4}}= 1.657$，$\Delta_{1,2} = 1.125 < CD，\Delta_{2,3} = 0.75 < CD，\Delta_{1,3} = 1.775 > CD$则算法A与算法C的差异较大，其它算法之间没有明显差异

## 2.6 偏差与方差

对于回归问题，令$Y = f(X) + \epsilon$，其中$Y$为真实值，$f(X)$为预测值，$\epsilon$为泛化误差, 且有$E(\epsilon) = 0, Var(\epsilon) = \sigma_{\epsilon}^2$

对于泛化误差，有如下偏差-方差分解(讨论回归问题)：
$$
\begin{array}{ll}
Err(x_0)\\
 = E[Y - \hat{f}(x_0)^2|X = x_0] \\
 = \sigma_{\epsilon}^2 + [E\hat{f}(x_0) - f(x_0)]^2 + E[\hat{f}(x_0) - E\hat{f}(x_0)]^2 \\
 = \sigma_{\epsilon}^2 + Bias^2(\hat{f}(x_0)) + Var(\hat{f}(x_0)) \\
 = Irreducible Error + Bias^2 + Variance
\end{array}
$$
其中$\sigma_{\epsilon}^2$(噪声)是不可避免的，也是泛化误差的最小下界

一般来说，偏差和方差存在冲突（bias-variance dilemma）

![image-20200408000227693](C:\Users\mi\AppData\Roaming\Typora\typora-user-images\image-20200408000227693.png)

* 偏差度量学习算法的期望预测与真实结果的偏离程度，刻画算法的拟合能力
* 方差度量训练集数据变动导致的学习性能的变化

* 随着训练程度增大，拟合逐渐增强，偏差减少，数据扰动被模型感知，方差增大